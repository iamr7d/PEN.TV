#!/usr/bin/env python3
"""
news_collector.py

Lightweight RSS/Atom headline collector with zero external dependencies.
- Fetches a set of feeds
- Extracts titles (RSS <item><title> and Atom <entry><title>)
- Deduplicates and limits
- Writes two files in the same folder as this script:
  * ticker_data.js   -> window.TICKER_TEXT / window.TICKER_ITEMS / window.TICKER_UPDATED_AT
  * news_cache.json  -> JSON with items and metadata

Usage:
  python news_collector.py              # one-off update
  python news_collector.py --interval 120  # update continuously every 120s

This is designed to work with OBS Browser Source loading the HTML from the same folder.
Placing <script src="ticker_data.js"></script> in your HTML will make the saved headlines
available even when offline.
"""
from __future__ import annotations

import argparse
import json
import sys
import time
import urllib.request
import urllib.error
import xml.etree.ElementTree as ET
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Set

# Same feeds as used in the HTML (can be customized)
RSS_FEEDS = [
    # General
    'https://feeds.bbci.co.uk/news/rss.xml',
    'https://www.reuters.com/rssFeed/topNews',
    'https://apnews.com/hub/apf-topnews?output=rss',
    'https://www.aljazeera.com/xml/rss/all.xml',
    # Tech
    'https://www.theverge.com/rss/index.xml',
    'https://techcrunch.com/feed/'
]

MAX_ITEMS = 60
TIMEOUT = 8  # seconds
UA = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'


def _fetch(url: str) -> str:
    req = urllib.request.Request(url, headers={'User-Agent': UA})
    with urllib.request.urlopen(req, timeout=TIMEOUT) as resp:
        data = resp.read()
    # Try to decode as utf-8; fall back to latin-1
    try:
        return data.decode('utf-8', errors='replace')
    except Exception:
        return data.decode('latin-1', errors='replace')


def _parse_titles(xml_text: str) -> List[str]:
    if not xml_text:
        return []
    try:
        root = ET.fromstring(xml_text)
    except ET.ParseError:
        return []

    # RSS: channel/item/title
    titles = []
    for item in root.findall('.//item'):
        t = item.findtext('title')
        if t:
            titles.append(t.strip())

    if titles:
        return titles

    # Atom: entry/title
    for entry in root.findall('.//{*}entry'):
        # Atom often namespaces title
        title_el = entry.find('{*}title')
        if title_el is not None and title_el.text:
            titles.append(title_el.text.strip())

    return titles


def collect_headlines(feeds: List[str]) -> List[str]:
    all_titles: Set[str] = set()
    for url in feeds:
        try:
            xml_text = _fetch(url)
            titles = _parse_titles(xml_text)
            for t in titles[:10]:
                tt = ' '.join(t.split())  # collapse whitespace
                if tt:
                    all_titles.add(tt)
            if len(all_titles) >= MAX_ITEMS:
                break
        except (urllib.error.URLError, urllib.error.HTTPError, TimeoutError, Exception):
            # Ignore individual feed errors; continue with others
            continue

    return list(all_titles)[:MAX_ITEMS]


def write_outputs(items: List[str], out_dir: Path) -> None:
    ts = datetime.now(timezone.utc).isoformat()
    sep = ' â€¢ '
    joined = (sep.join(items) + (sep if items else '')).strip()

    # Write JSON
    json_path = out_dir / 'news_cache.json'
    json_tmp = out_dir / 'news_cache.json.tmp'
    with json_tmp.open('w', encoding='utf-8') as f:
        json.dump({
            'updated_at': ts,
            'count': len(items),
            'items': items,
            'text': joined,
        }, f, ensure_ascii=False, indent=2)
    json_tmp.replace(json_path)

    # Write JS (to be loaded by HTML)
    js_path = out_dir / 'ticker_data.js'
    js_tmp = out_dir / 'ticker_data.js.tmp'
    with js_tmp.open('w', encoding='utf-8') as f:
        f.write('// Auto-generated by news_collector.py\n')
        f.write(f'window.TICKER_UPDATED_AT = {json.dumps(ts)};\n')
        f.write(f'window.TICKER_ITEMS = {json.dumps(items, ensure_ascii=False)};\n')
        f.write(f'window.TICKER_TEXT = {json.dumps(joined, ensure_ascii=False)};\n')
    js_tmp.replace(js_path)



def main(argv: List[str]) -> int:
    parser = argparse.ArgumentParser(description='Collect news headlines and write ticker data.')
    parser.add_argument('--interval', type=int, default=0, help='Seconds between updates (0 = run once)')
    parser.add_argument('--limit', type=int, default=MAX_ITEMS, help='Max items to keep')
    args = parser.parse_args(argv)

    global MAX_ITEMS
    MAX_ITEMS = max(1, args.limit)

    out_dir = Path(__file__).resolve().parent

    def run_once():
        items = collect_headlines(RSS_FEEDS)
        write_outputs(items, out_dir)
        print(f"[news_collector] Saved {len(items)} items at {datetime.now().isoformat(timespec='seconds')}")

    if args.interval > 0:
        while True:
            run_once()
            time.sleep(max(10, args.interval))
    else:
        run_once()
    return 0


if __name__ == '__main__':
    raise SystemExit(main(sys.argv[1:]))
